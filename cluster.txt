#######################################################################

			集群与存储  Day01

			————存储技术与应用————

				1.存储的目标
@存储是根据不同的应用环境通过采取合理、安全、有效的方式将数据保存到某些介质上并能保证有效的访问

@一方面它是数据临时或长期驻留的物理媒介

@另一方面,它是保证数据完整安全存放的方式或行为

@存储就是把这两个方面结合起来,向客户提供一套数据存放解决反感

				2.存储技术分类
-SCSI小型计算机系统接口		-DAS直连式存储
-NAS网络技术存储		-SAN存储区域网络
-FC光纤通道

			————常见存储技术————

				1.SCSI技术
@Small Computer System Interface的简称

@作为输入/输出接口

@主要用于硬盘、光盘、磁带机等设备

				2.DAS技术
@Direct-Attached Storage的简称

@将存储设备通过SCSI接口或光纤通道直接连接到计算机上

@不能实现数据与其他主机的共享

@占用服务器操作系统资源,如CPU、IO等

@数据量越大,性能越差


				3.NAS技术
@Network-Attached Storage的简称

@一种专用数据存储服务器,以数据为中心,将存储设备与服务器彻底分离,几种管理数据,从而释放带宽、提高性能、降低总拥有成本、保护投资

@用户通过TCP/IP协议访问数据
  -采用标准的NFS/HTTP/CIFS等


				4.SAN技术
@Storage Area Network的简称
  -通过光纤交换机、光纤路由器、光纤集线器等设备将磁盘阵列、磁带等存储设备与相关服务器连接起来,形成高速专网网络

@组成部分
  -如路由器、光纤交换机
  -接口:如SCSI、FC
  -通信协议:如IP、SCSI

@Fibre Channel
  -一种是用于千兆数据传输的、成熟而安全解决方案
  -与传统的SCSI相比,FC提供更高的数据传输速率、更远的传输距离、更多的设备连接支持以及更稳定的性能、更简易的安装

@FC主要组件
  -光纤		-HBA(主机总线配置器)		-FC交换机

@FC交换机交换拓扑
  -点到点:point-to-point		#简单将两个设备互连
  -已裁定的环路:arbitrated loop		#可多达126个设备共享一段信道或环路
  -交换式拓扑:switched fabric		#所有设备通过光纤交换机互连


				5.ISCSI技术
@Internet SCSI

@IETF制定的标准,将SCSI数据块映射为以太网数据包

@是一种基于IP Storage理论的新型存储技术

@将存储行业广泛应用的SCSI接口技术与IP网络相结合

@可以再IP网络上构建SAN

@最初由Cisco和IBM开发

@优势
-基于IP协议技术的标准
-允许网络再TCP/IP协议上传输SCSI命令
-相对FC SAN,ISCSI实现的IP SAN投资更低
-解决了传输效率、存储容量、兼容性、开放性、安全性等方面的问题
-没有距离限制

@客户端
-ISCSI Initiator:软件实现,成本低、性能较低
-ISCSI HBA:硬件实现,性能好,成本较高

@存储设备端
-ISCSI Target

@以太网交换机


			————ISCSI技术应用————

			——————基础知识——————

				1.ISCSI操作流程
@Target端
-选择target名称		-安装ISCSI target	-准备用于target的存储
-配置target		-启用服务

@Initiator端
-安装initiator		-配置initiator并启动服务

				2.ISCSI命名规范
@建议采用IQN(ISCSI限定名称)

@全称必须全局唯一

@IQN格式:
iqn.<日期代码>.<反向域>.<字符串>[:<字串>]

@命名示例
iqn.2013-01.com.tarena.tech:sata.rack2.disk1

			—————部署ISCSI服务————

			1.安装target软件
@查询yum仓库
] # yum list | grep target

@安装
] # yum -y install targetcli

@查看ISCSI target信息
] # yum info targetcli

			2.配置ISCSI Target
@定义后端存储
} # targetcli
/> ls
/> backstores/block create store /dev/vdb1

@创建iqn对象(服务器)
/> /iscsi create iqn.2018-01.cn.tedu:server1

@授权客户机访问
/> iscsi/iqn.2018-01.cn.tedu:server1/tpg1/acls create iqn.2018-01.cn.tedu:client1

@绑定存储
/> iscsi/iqn.2018-01.cn.tedu:server1/tpg1/luns create /backstores/block/iscsi_store

@绑定监听地址
/> iscsi/iqn.2018-01.cn.tedu:server1/tpg1/portals/ create 0.0.0.0

@保存配合子
/> saveconfig
/> exit

			3.服务管理

@控制服务
]# systemctl {start|restart|stop|status} target

@设置服务开机运行
]# systemctl enable target

@查看端口
]# netstat -ntulp | grep :3260

			4.安装客户端软件
@查询yum仓库
]# yum list | grep initiator

@安装
]# yum -y install iscsi-initiator-utils

@查看ISCSI target信息
]# yum info iscsi-initiator-utils

			5.客户端访问并验证
@启动服务
]# service iscsi start

@设置本机的iqn名称
]# vim /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2018-01.cn.tedu:client1

@发现远程target存储
]# iscsiadm --mode discoverydb --type sendtargets --portal 192.168.4.1 --discover

@登陆target
]# systemctl restart iscsi
]# lsblk
]# systemctl enable iscsi
]# systemctl enable iscsid


			————udev配置————

			————基础知识————

			1.设备文件管理方法
@devfs
-Linux早期采用的静态管理方法
-/dev目录下由大量静态文件
-内核版本2.6.13开始被完全取代

@udev
-只有连到系统上来的设备才在/dev下创建设备文件
-与主、次设备编号无关
-为设备提供持久、一致的名字

			2.接入设备事件链

-内核发现设备并导入设备状态到sysfs
-udev接到事件通知
-udev创建设备节点或是运行指定程序
-udev通知hald守护进程
-HAL探测设备信息
-HAL创建设备对象结构
-HAL通过系统消息总线广播该事件
-用户程序也可以监控该事件

			3.udev的作用

@从内核收到添加/移除硬件事件时,udev将会分析:
-/sys目录下信息		-/etc/udev/rules.d目录中的规则

@基于分析结果,udev会:
-处理设备命名		-决定要创建哪些设备文件或链接
-决定如何设置属性	-决定触发哪些事件

			————udev应用————

			1.udev事件实时监控
]# udevadm monitor --property		#实时同步过程中插入存储设备生效

			2.配置udev
@主配置文件/etc/udev/udev.conf
-udev_root:创建设备文件位置,默认为/dev
-udev_rules:udev规则文件位置,默认为/etc/udev/rules.d
-udev_log:syslog优先级,缺省为err

@文件位置及格式
-/etc/udev/rules.d/<rule_name>.rules
-例:75-custom.rules

@规则格式
-<match-key><op><value>[,...]<assignment-key><op>value[,...]
-例:BUS=="usb",SYSFS{serial}=="20043512321411d34721",NAME="udisk"

@操作符(匹配)
-  ==:表示匹配
-  !=:表示不匹配
@匹配示例
-  ACTION=="add"
-  KERNEL=="sd[a-z]1"
-  BUS=="scsi"
-  DRIVER!="ide-cdrom"
-  PROGRAM=="myapp.pl",RESULT=="test"

@操作符(值)
-  =:指定赋予的值
-  +=:添加新值
-  ：=：指定值,且不允许被替换
@示例
-  NAME="udisk"
-  SYMLINK+="data1"
-  OWNER="student"
-  MODE="0600"

				3.udev变量
@可以简化或缩写规则
KERNEL=="sda*",SYMLINK+="iscsi%n"

@常用替代变量
-  %k:内核所识别出来的设备名,如sdb1
-  %n:设备的内核编号,如sda3中的3
-  %p:设备路径,如/sys/block/sdb/sdb1
-  %%:%符号本身

			————Multipath多路径————
			1.多路径概述
@当服务器到某一存储设备有多条路径时,每条路径都会识别为一个单独的设备
@多路径允许您将服务器节点和存储阵列间的多个I/O路径配置为一个单以设备
@这些I/O路径是可包含独立电缆、交换器和控制器的尸体SAN链接
@多路径集合了I/O路径,并生成这些集合路径组成的新设备

			2.多路径主要功能
@冗余				@改进的性能
-主备模式,高可用		-主主模式,负载均衡

			————配置案例————
			1.拓扑规划
@利用iscsi实现多路径
	应用服务器			IP地址
	eth0				192.168.1.10/24
	eth1				192.168.2.10/24

	存储节点			IP地址
	eth0				192.168.1.20/24
	eth1				192.168.2.20/24

			2.准备共享存储
@配置iSCSI服务端
-准备共享介质(分区、LV或磁盘镜像)
-安装scsi-target-utils
-准备规划iqn名称
-修改target.conf配置文件,以提供存储
-启动tgtd服务

			3.访问共享存储
@因为到达共享存储有两条路径,所以需要在两条路径上都执行发现命令
]# iscsiadm --mode discovery --type sendtargets --portal 192.168.1.20 --discover
]# iscsiadm --mode discovery --type sendtargets --portal 192.168.2.20 --discover

@设置开机自启
]# systemctl enable iscsi
]# systemctl enable iscsid

			4.多路径设备
@若没有DM Multipath,从服务器节点到储存控制器的每一条路径都会被系统视为独立的设备,即使I/O路径连接的是相同的服务器节点到相同的储存控制器也是如此
@DM Multipath提供了有逻辑的管理I/O路径的方法,即在基础设备顶端生成单一多路径设备

			5.多路径配置概述
@安装软件包
]# yum -y install device-mapper-multipath
@使用mpathconf命令创建配置文件并启用多路径
]# mpathconf --user_friendly_names n
@若无需要编辑该配置文件,可使用此命令启动多路径守护程序

			6.多路径设备识别符
@每个多路径设备都有一个WWID(全球识别符),它是全球唯一的、无法更改的号码
@默认情况下会将多路径设备的名称设定为它的WWID
@可以在多路径配置文件中设置user_friendly_names选项,该选项可将别名设为格式为mpathn的节点唯一名称
@也可以自定义存储设备名称

			7.获取WWID
@假如共享存储在本地被识别为/dev/sdb和/dev/sdc,那么获取它WWID的方法是:
]# /lib/udev/scsi_id --whitelisted --device=/dev/sdb
]# /lib/udev/scsi_id --whitelisted --device=/dev/sdc
@因为两个设备虽然名称不一样,但是实际上是一个设备,所以他们的WWID是相同的

			8.指定获取WWID的方法
@在配置文件中色横名获取WWID的方法
]# vim /etc/multipath.conf
defaults{
user_friendly_names no
find_multipaths yes
}

			9.为多路径设备配置别名
@根据得到的WWID,为多路径设备配置别名
]# vim /etc/multipath.conf
#在尾部添加以下内容:
multipath{
multipath{
wwid "1IET  00010001"
alias mpatha
}
}

			10.启动服务并验证
@启动服务
]# systemctl start multipathd
]# systemctl enable mulstipathd
@验证
]# ls /dev/mapper		#maptha即为多路径设备
]# multipath -rr		#重新加载多路径信息
]# multipath -ll		#查看多路径信息
@分区
-为/dev/mapper/mpath分区,得到的第一个分区名为/dev/mapper/mpathap1



#######################################################################

			————集群与存储 Day02————

				————集群————
				
				1.什么是集群
@一组通过告诉网络互联的计算组,并以单一系统的模式加以管理
@将很多服务器几种起来一起,提供同一种服务,再客户端看来就像是只有一个服务器
@可以在付出较低成本的情况下获得在性能、可靠性、灵活性方面的相对较高的收益
@任务调度是集群系统中的核心技术

				2.集群目的
@提高性能
-如计算密集型应用,如:天气预报、核试验模拟
@降低成本
-相对百万美元级的超级计算机,价格便宜
@提高可扩展性
-只要增加集群节点即可
@增强可靠性
-多个节点完成相同功能,避免单点失败

				3.集群分类
@高性能计算集群HPC
-通过以集群开发的并行应用程序,解决复杂的科学问题
@负载均衡(LB)集群
-客户端负载在计算机集群中尽可能平均分摊
@高可用(HA)集群
-避免单点故障,当一个系统发生故障时,可以快速迁移

				————LVS————

				1.LVS项目介绍
@Linux虚拟服务器(LVS)是章文嵩在国防科技大学就读博士期间创建的
@LVS可以实现高可用的、可伸缩的Web、Mail、Cache和Media等网络服务
@最终目标是利用Linux操作系统和LVS集群软件时先一个高可用、高性能、低成本的服务器应用集群

				2.LVS集群组成
@前端:负载均衡层
-由一台或多台负载调度器构成
@中间:服务器集群层
-由一组实际运行应用服务的服务器组成
@底端:数据共享存储层
-提供共享存储空间的存储区域

				3.LVS术语
@Director Server:调度服务器
-将负载分发到Real Server的服务器
@Real Server:真实服务器
-真正提供应用服务的服务器
@VIP:虚拟IP地址
-公布给用户访问的虚拟IP地址
@RIP:真实IP地址
-集群节点上使用的IP地址
@DIP:调度器连接节点服务器的IP地址

				4.LVS工作模式
@VS/NAT
-通过网络地址转换实现的虚拟服务器
-大并发访问时,调度器的性能成为瓶颈
@VS/DR
-直接使用路由技术时先虚拟服务器
-节点服务器需要配置VIP,注意MAC地址广播
@VS/TUN
-通过隧道方式实现虚拟服务器

				5.负载均衡调度算法
@LVS目前实现了10种调度算法
@常用调度算法有4种
-轮询(Round Robin)
-加权轮询(Weighted Round Robin)
-最少连接(Least Connections)
-加权最少连接(Weighted Least Connections)

@轮询(Round Robin)
-将客户端请求平均分法到Real Server

@加权轮询(Weighted Round Robin)
-根据Real Server权重值进行轮询调度

@最少连接(Least Connections)
-选择连接数最少的服务器

@加权最少连接(Weighted Least Connections)
-根据Real Server权重值,选择连接数最少的服务器

@源地址散列(Source Hashing)
-根据请求的目标IP地址,作为散列键(Hash Key)从静态分配的散列表找出对应的服务器

@其他调度算法
-基于局部性的最少链接
-带复制的基于局部性最少链接
-目标地址散列(Destination Hashing)
-最短的期望的延迟
-最少队列调度

			————安装软件————

			1.安装前准备
@LVS的IP负载均衡技术是通过IPVS模块时先的
@IPVS模块已成为Linux组成部分

			2.安装ipvsadm
@使用yum命令安装ipvsadm(或者rpm)

			3.ipvsadm用法
@创建虚拟服务器
—— -A		#添加虚拟服务器
—— -t		#设置群机地址(VIP,Virtual IP)
—— -s		#指定负载调度算法

@添加、删除服务器节点
—— -a		#添加真实服务器
—— -d		#删除真实服务器
—— -r		#指定真实服务器(Real Server)的地址
—— -m		#使用NAT模式;-g、-i分别对应DR、TUN模式
—— -w		#为节点服务器设置权重,默认为1

@查看IPVS
—— ipvsadm -Ln

			————LVS-NAT案例————

			1.操作流程
@Real Server:
-配置WEB服务器
@Director Server
-在调度器上安装并启用ipvsadm	-创建虚拟服务器 	
-向虚拟服务器中加入节点

@Client:
-连接虚拟服务器测试

			2.部署LVS调度器
@打开ip_forward
]# vim /etc/sysctl.conf		#写入配置文件,开机自起
net.ipv4.ip_forward = 1
]# sysctl -p			#激活

@启动ipvsadm
]# systemctl start ipvsadm
]# systemctl enable ipvsadm

@创建虚拟服务器,VIP为10.10.10.1,采用的调度算法为Round Robin
]# ipvsadm -A -t 10.10.10.1:80 -s rr	#VIP地址可任意设定(非本机IP)

@向虚拟服务器中加入节点,并指定权重分别为1和2,目前权重不起作用(为什么?)
]# ipvsadm -a -t 10.10.10.1:80 -r 192.168.10.11 -m -w 1
]# ipvsadm -a -t 10.10.10.1:80 -r 192.168.10.12 -m -w 2

@查看配置
]# ipvsadm -Ln

@保存配置
]# ipvsadm -save(或-S) > /etc/sysconfig/ipvsadm(保存规则到文档,开机自起)

@修改Director调度算法为WRR
]# ipvsadm -E -t 10.10.10.1:80 -s wrr

			3.Client(客户)端验证
@通过web浏览器访问
@使用ab进行大并发测试
]# ab -c 10 -n 1000 http://10.10.10.1/index.html
@在Director上查看连接数
]# ipvsadm -Ln

			————LVS-DR集群————

			1.操作流程
@Real Server:
-配置WEB服务器		-配置辅助IP地址、调整内核参数
@Director Server:
-在调度器上安装并启用ipvsadm		-配置辅助IP地址
-创建虚拟服务器、向虚拟服务器中加入节点
@Client
-连接虚拟服务器测试

			2.ARP广播的问题
@当库户端发起访问VIP对应的域名的请求时,根据网络通信原理会产生ARP广播
@因为负载均衡器和真实的服务器在同一网络并且VIP设置在集群中的每个节点上
@此时集群内的真实服务器会尝试会带来自客户端的ARP广播,着就会产生问题,大家都说我是"VIP"

			3.内核参数说明
@arp_ignore(定义回复ARP广播的方式)
—— 0(默认值)
	#回应所有的本地地址ARP广播,本地地址可以配置在任意网络接口
—— 1
	#只回应配置在入站网卡接口上的任意IP地址的ARP广播

@arp_announce
—— 0(默认值)
	#使用配置在任意网卡接口上的本机IP地址
—— 2
	#对查询目标使用最适当的本地地址.在次模式下将忽略这个IP数据包的源地址并尝试选择与能该地址通信的本地地址.首要是选择所有的网络接口的子网中外出访问子网中包含该目标IP地址的本地地址.如果没有和是的地址被发现,向选择当前的发送网络接口或其他的有可能接受到该ARP回应的网络接口来进行发送

			4.ARP防火墙(扩展内容)




			————LVS-DR案例实施————

			1.配置后端Web服务器
@配置辅助VIP地址
]# ifconfig lo:1 192.168.10.100/24 
@调整内核参数
]# echo 1 > /proc/sys/net/ipv4/conf/lo/arp_ignore
]# echo 2 > /proc/sys/net/ipv4/conf/lo/arp_announce
]# echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
]# echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce

			2.配置LVS调度器
@配置辅助IP地址
]# ifconfig eth0:1 192.168.10.100/24
@创建虚拟服务器
]# ipvsadm -A -t 192.168.10.100:80 -s wlc
@向虚拟服务器中加入节点
]# ipvsadm -a -t 192.168.10.100:80 -g -r 192.168.10.11 -w 1
]# ipvsadm -a -t 192.168.10.100:80 -g -r 192.168.10.12 -w 2

			3.Client(客户)端验证
@通过web浏览器访问
@使用ab进行大并发测试
]# ab -c 10 -n 1000 http://10.10.10.1/index.html
@在Director上查看连接数
]# ipvsadm -Ln


#######################################################################

			————集群与管理 Day03————

			————Keepalived热备份————

			1.Keepalived概述
@调度器出现单点故障,如何解决?
@Keeplived实现了高可用集群
@Keeplived最初是为LVS设计的,专门监控个服务器节点的状态
@Keepalived后来加入了VRRP功能,防止单点故障

			2.Keepalived运行原理
@Keepalived检测每个服务器节点状态
@服务器节点异常或工作出现故障,Keepalived将故障节点从集群系统中剔除
@故障节点恢复后,Keepalived再将其加入到集群系统中
@所有工作自动完成,无需人工干预

			————Keepalived服务————

			1.Keepalived安装
@RHEL7的光盘中已经包含Keepalived软件包,只要配置好yum,指向光盘源即可安装
]# yum -y install keepalived

			2.配置文件解析
@主配置文件:/etc/keepalived/keepalived.conf
global_defs{
notification_email{
admin@tarena.com.cn			#设置报警收件人邮箱(可修改)
}
notification_email_from ka@localhost	#设置发件人
smtp_server 192.168.20.1		#定义邮件服务器
smtp_connect_timeout 30			#设置连接超时时间,以秒为单位
router_id LVS_devel			#设置路由ID号
}
vrrp_instance Vl_1{			#Vl-1为服务器名,可修改
state MASTER				#主服务器为MASTER,辅助为BACKUP
interface eth0				#定义网络接口
virtual_router_id 51			#主辅VRID号必须一致
priority 100				#服务器优先级
advert_int 1
authentication {
auth_type pass				#设置以密码的方式验证
auth_pass forlvs			#主辅服务器必须密码一致
}
virtual_ipaddress { 192.168.20.100 }
}

			3.高可用拓扑
@使用Keepalived为主从设备提供VIP地址漂移
 ———————————————————————————————————————
|    ——————————————————————————         |
|   |——————————————————————————————————————————————
|   |    Keepalived:           |        |          |
|  ———  VIP:192.168.20.1/24    ———      |       ————————
| |web|                       |web|     |      |存储设备|
|  ———   Web Server:           ———      |       ————————
|       192.168.20.101-102/24           |
|                                       |
 ———————————————————————————————————————

			4.高可用Web案例
@配置Web服务器
1]# ifconfig eth0 192.168.20.101
1]# yum -y install httpd
1]# systemctl start httpd;systemctl enable httpd
2]# ifconfig eth0 192.168.20.102
2]# yum -y install httpd 
2]# systemctl start httpd;systemctl enable httpd

@使用Keepalived为服务器提供VIP
-写好配置文件,启动服务后即可使用VIP

			————Keepalived+LVS————

			————高可用调度器————
			
			1.Keepalived+LVS拓扑
@使用Keepalived高可用解决调度器单点失败问题
@主、备调度器上配置LVS
@主调度器异常时,Keepalived启用备用调度器

			2.Keepalived配置说明
@LVS相关信息通过Keepalived配置即可
@主要配置文件说明如下：
global_defs{
notification_email{
admin@tarena.com.cn                     #设置报警收件人邮箱(可修改)
}
notification_email_from ka@localhost    #设置发件人
smtp_server 192.168.20.1                #定义邮件服务器
smtp_connect_timeout 30                 #设置连接超时时间,以秒为单位
router_id LVS_devel                     #设置路由ID号
}

@VRRP实例设置
vrrp_instance Vl_1{			#Vl-1为服务器名,可修改
state MASTER				#主服务器为MASTER,辅助为BACKUP
interface eth0				#定义网络接口
virtual_router_id 51			#主辅VRID号必须一致
priority 100				#服务器优先级
advert_int 1
authentication {
auth_type pass				#设置以密码的方式验证
auth_pass forlvs			#主辅服务器必须密码一致
}
virtual_ipaddress { 192.168.20.100 }
}
virtual_server 192.168.20.100 80{	#设置VIP为192.168.20.100
delay_loop 6
lb_algo rr				#设置LVS调度算法为RR
lb_kind DR				#设置LVS的模式为DR
persistence_timeout 50
protocol TCP
real_server 192.168.20.150 80{
weight 3				#设置权重为3
TCP_CHECK {
connect_timeout 3
nb_get_retry 3
delay_before_retry 3
}
}
real_serveer 192.168.20.151 80 {同real1}
}

			3.Real Server配置
@真实服务器运行在DR模式下	@修改内核参数,并附加VIP
@详细配置参见LVS相关章节

		
			————HAProxy服务器————
			
			1.HAProxy简介
@它是免费、快捷并且可靠的一种解决方案
@适用于那些负载特大的web站点,这些站点通常又需要会话保持或七层处理
@提供高可用性、负载均衡以及基于TCP和HTTP应用的代理

			2.衡量负责均衡器性能的因素
@Session rate 会话率
-每秒钟产生的会话数
@Session concurrency 并发会话数
-服务器处理会话的时间越长,并发会话数越多
@Data rate 数据速率
-以MB/s或Mbps衡量
-大的对象导致并发会话数增加
-高会话数、高数据速率要求更多的内存

			3.HAProxy工作模式
@mode http
-客户端请求被深度分析后再发往服务器
@mode tcp
-客户端与服务器之间建立会话,不检查第七层信息
@mode health
-仅做健康状态检查,已经不建议使用

			————HTTP协议解析————
			
			1.HTTP解析
@当HAProxy运行再HTTP模式下,HTTP请求(Request)和响应(Response)均被完全分析和索引,这样便于创建恰当的匹配规则
@理解HTTP请求和响应,对于更好的创建匹配规则至关重要

			2.HTTP事务模型
@HTTP协议是事务驱动的
@每个请求(Request)仅能对应一个响应(Response)
@常见模型:
—— HTTP close		—— Keep-alive		—— Pipelining

@HTTP close
-客户端向服务器建立一个TCP连接
-客户端发送请求给服务器
-服务器响应客户端请求后即断开连接
-如果客户端到服务器的请求不只一个,那么就要不断的去建立连接
-TCP三次握手消耗相对较大的系统资源,同时延迟较大

@Keep-alive
-一次连接可以传输多个请求
-客户端需要知道传输内容的长度,以避免无限期的等待传输结束
-降低两个HTTP事务间的延迟
-需要相对较少的服务器资源

@Pipelining
-仍然使用Keep-alive
-在发送后续请求前,不用等前面的请求已经得到回应
-适用于有大量图片的页面
-降低了多次请求之间的网络延迟

			3.HTTP头部信息
@请求头部信息
-方法:GET
-URI:/serv/login.php?lang=en&profile=2
-版本:HTTP/1.1

@请求头部信息
-请求头部包含许多有关的客户端环境和请求正文的有用信息,如浏览器所使用的语言、请求正文的长度等

@响应头部信息
-版本:HTTP/1.1
-状态码:200
-原因:OK


			————HAProxy配置实例————

			1.项目拓扑图
 —————————————————————————————————————————————————
|   | ———————————————————————————————|             |
|   |————————————————|———————————————|————————————|—————————
|   |                |               |            |         |
| —————             ————            ————          |      ————————
|| HAP |           |web1|          |web2|         |     |存储设备|
| —————             ————            ————          |      ————————
|HAProxy                Web Server:               |
|192.168.20.1/24      192.168.20.101-102/24       | 
 —————————————————————————————————————————————————

			2.HAProxy安装
@RHEL7光盘中内置了HAProxy,只要配置好yum,可以直接安装

			3.配置文件说明
@HAProxy配置参数来源
-命令行:总是具有最高优先级
-global部分:全局设置进程级别参数
-代理声明部分		#来自于default、listen、frontend和backend

@配置文件可由如下部分构成:
-default
#为后续的其他部分设置缺省参数
#缺省参数可以被后续部分重置
-frontend
#描述接收客户端侦听套接字(socket)集
-backend
#描述转发链接的服务器集
-listen
#把frontend和backend结合到一起的完整声明

@/etc/haproxy/haproxy.cfg
global
log 127.0.0.1 local2 		#调试错误警告信息日志
chroot /usr/local/haproxy
pidfile /var/run/haproxy.pid	#haproxy的pid存放路径
maxconn 4000			#最大连接数,默认4000
user haproxy
group haproxy
daemon				#创建1个进程进入deamon模式运行

defaults
mode http			#默认的模式mode(tcp|http|health)
log global			#采用全局定义的日志
option dontlognull		#簿记录健康检查的日志信息
option httpclose		#每次请求完毕后主动关闭http通道
option httplog			#日志类别http日志格式
option forwardfor		#后端服务器可以从Http Header中获得客户端ip
option redispatch		#serverid服务器挂掉后强制丁香到其他健康服务器
timeout connect 10000		#如果backend没有指定,默认为10s
timeout client 300000		#客户端连接超时时间		
timeout server 300000		#服务器连接超时时间
maxconn 60000			#最大连接数
retries 3			#3次连接失败就认为服务不可用,也可以通过后面设置

listen stats
bind 0.0.0.0:1080		#监听端口
stats refresh 30s		#统计页面自动刷新时间
stats url /stats		#统计页面url
stats realm Haproxy Manager	#统计页面密码框上提示文本
stats auth admin:admin		#统计页面用户名和密码设置
#stats hide-version		#隐藏统计页面上HAProxy的版本信息

listen websrv-rewrite 0.0.0.0:80
cookie SERVERID rewrite
balance roundrobin
server web1 192.168.20.101:80 cookie \
app1inst1 check inter 2000 rise 2 fall 5
server web2 192.168.20.102:80 cookie \
app1inst2 check inter 2000 rise 2 fall 5

			4.管理服务
@启动服务
systemctl start haproxy
@停止服务
systemctl stop haproxy
@查看状态
systemctl status haproxy

			————集群调度软件对比————

			1.Nginx分析
@优点
-工作在7层,可以针对http做分流策略
-正则表达式比HAProxy强大
-安装、配置、测试简单,通过日志可以解决多数问题
-并发量可以达到几万次
-Nginx还可以作为Web服务器使用
@缺点
-仅支持http、https、mail协议,应用面小
-监控检查仅通过端口,无法使用url检查

			2.LVS分析
@优点
-负载能力强,工作在4层,对内存、CPU消耗低
-配置性低,没有太多可配置性,减少人为错误
-应用面广,几乎可以为所有应用提供负载均衡
@缺点
-不支持正则表达式,不能实现动静分离
-如果网站架构庞大,LVS-DR配置比较繁琐

			3.HAProxy分析
@优点
-支持session、cookie功能
-可以通过url进行健康检查
-效率、负载均衡速度,高于Nginx,低于LVS
-HAProxy支持TCP,可以对MySQL进行负载均衡
-调度算法丰富
@缺点
-正则弱于Nginx
-日志依赖于syslogd,不支持apache日志


#######################################################################

			————集群与存储 Day04————

			————Ceph————

			————基础知识————

			1.什么是分布式文件系统
@分布式文件系统(Distributed File System)是指文件系统管理的物理存储资源不一定直接在本地节点上,二是通过计算机网络与节点相连
@分布式文件系统的设计集于客户机/服务器模式

			2.常用分布式文件系统
@Lustre	   @Hadoop	@FastDFS	@Ceph		@GlusterFS

			3.什么是Ceph
@Ceph是一个分布式文件系统
@具有高扩展、高可用、高性能的特点
@Ceph可以提供对象存储、块存储、文件系统存储
@Ceph可以提供PB级别的存储空间(PB->TB->GB)
-1024G*1024G=1048576G
@软件定义存储(Software Defined Storage)作为存储行业的一大发展趋势,已经越来越受到市场的认可
			
			4.Ceph组件
@OSDs		@Monitors	#MDSs			@Client
-存储设备	-集群监控组件	-存放文件系统的元数据	-ceph客户端

			————实验环境准备————

			1.实验拓扑图
@1台客户端虚拟机	@3台存储集群虚拟机
			     ————
		   ———————— |虚拟|  node1
		  |	     ————   eth0:192.168.4.11
		  |
   ————		  |	     ————
  |客户|—————————— ———————— |虚拟|  node2
   ————		  |	     ————   eth0:192.168.4.12
client            |
eth0:192.168.4.10 |	     ————
		   ———————— |虚拟|  node3
			     ————   eth0:192.168.4.13

			2.配置YUM
			3.配置SSH无密钥连接(包括本机)
			4.NTP时间同步(以客户机为主机)
			5.准备存储磁盘
			6.配置域名解析
			
			————部署Ceph集群————

			————准备部署环境————

			1.安装部署软件
@使用node1作为部署主机
]# yum -y install ceph-deploy
@ceph-deploy命令与子命令都支持--help查看帮助
]# ceph-deploy --help
			
			2.创建目录
@为部署工具创建目录,存放密钥与配置文件
]# mkdir /root/ceph-cluster
]# cd ceph-cluster

			————部署存储集群————
			
			1.创建Ceph集群(在集群管理主机上操作)
@创建Ceph集群配置(所有节点都为mon)
]# ceph-deploy new node1 node2 node3
@给所有节点安装Ceph软件包
]# ceph-deploy install node1 node2 node3
@初始化所有节点的mon服务(主机名解析必须对)

			2.创建OSD
@所有节点准备磁盘分区(每台节点主机都要做)
]# parted /dev/vdb mklabel gpt
]# parted /dev/vdb mkpart primary 1M 50%
]# parted /dev/vdb mkpart primary 50% 100%

]# chown ceph.ceph /dev/vdb1
]# chown ceph.ceph /dev/vdb2

@初始化清空磁盘数据(仅集群管理主机操作即可)
]# ceph-deploy disk zap node1:vdc node1:vdd
]# ceph-deploy disk zap node2:vdc node2:vdd
]# ceph-deploy disk zap node3:vdc node3:vdd
@创建OSD存储空间(仅集群管理主机操作即可)
]# ceph-deploy osd create node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
]# ceph-deploy osd create node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
]# ceph-deploy osd create node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2

			3.验证
@查看集群状态
]# ceph -s
@可能出现的错误
-osd create创建OSD存储空间,如提示run 'gatherkeys'
]# ceph-deploy gatherkeys node1 node2 node3
-ceph -s查看状态,如果失败
]# systemctl restart ceph\*.service ceph\*.target
				#在所有节点,或仅在失败的节点重起服务

			————Ceph块存储————
			
			————概述————

			1.什么是块存储
@单机块存储
-光盘		-磁盘

@分布式块存储
-Ceph		-Cinder

@Ceph块设备也叫做RADOS块设备
-RADOS block device : RBD

@RBD驱动已经很好的集成了Linux内核中
@RBD提供了企业功能,如快照、COW克隆等等
@RBD还支持内存缓存,从而能够大大提高性能

@Linux内核可用直接访问Ceph块存储
@KVM可用借助于librbd访问

			————块存储集群————
		
			1.创建镜像
@查看存储池(默认有一个rbd池)
]# ceph osd lspools
@创建镜像、查看镜像
]# rbd create demo-image --image-feature layering --size 10G
]# rbd create rbd/image --image-feature layering --size 10G
]# rdb list			#查看镜像列表
]# rdb info demo-image		#查看镜像的详细信息

			2.集群内通过KRBD访问
@将镜像映射为本地磁盘
